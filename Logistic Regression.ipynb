{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.854</td>\n",
       "      <td>3.199</td>\n",
       "      <td>4.419</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>34.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.792</td>\n",
       "      <td>3.332</td>\n",
       "      <td>3.178</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.168</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.163</td>\n",
       "      <td>3.866</td>\n",
       "      <td>3.219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>61.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>3.240</td>\n",
       "      <td>5.247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0      11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1      33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2      23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3      38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4       7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "..      ...   ...      ...     ...  ...     ...    ...       ...       ...   \n",
       "195    55.0  44.0     24.0    83.0  1.0    23.0    0.0       1.0       0.0   \n",
       "196    34.0  23.0      3.0    24.0  1.0     7.0    0.0       1.0       0.0   \n",
       "197     6.0  32.0     10.0    47.0  1.0    10.0    0.0       1.0       0.0   \n",
       "198    24.0  30.0      0.0    25.0  4.0     5.0    0.0       1.0       1.0   \n",
       "199    61.0  50.0     16.0   190.0  2.0    22.0    1.0       1.0       1.0   \n",
       "\n",
       "     longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0       4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1       9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2       6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3       6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4       7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "..       ...  ...    ...       ...       ...     ...    ...      ...      ...   \n",
       "195    17.35  ...    0.0       0.0       0.0     1.0    0.0    2.854    3.199   \n",
       "196     6.00  ...    0.0       0.0       1.0     1.0    0.0    1.792    3.332   \n",
       "197     3.85  ...    0.0       0.0       1.0     1.0    0.0    1.348    3.168   \n",
       "198     8.70  ...    1.0       1.0       1.0     1.0    1.0    2.163    3.866   \n",
       "199    16.85  ...    0.0       1.0       0.0     0.0    1.0    2.824    3.240   \n",
       "\n",
       "     lninc  custcat  churn  \n",
       "0    4.913      4.0    1.0  \n",
       "1    3.497      1.0    1.0  \n",
       "2    3.401      3.0    0.0  \n",
       "3    4.331      4.0    0.0  \n",
       "4    4.382      3.0    0.0  \n",
       "..     ...      ...    ...  \n",
       "195  4.419      3.0    0.0  \n",
       "196  3.178      3.0    0.0  \n",
       "197  3.850      3.0    0.0  \n",
       "198  3.219      4.0    1.0  \n",
       "199  5.247      2.0    0.0  \n",
       "\n",
       "[200 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5cdb3dbbc92b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['churn'] = df['churn'].astype('int')\n"
     ]
    }
   ],
   "source": [
    "df= df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "df['churn'] = df['churn'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10)\n",
      "Index(['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',\n",
      "       'callcard', 'wireless', 'churn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Define X\n",
    "X = np.asarray(df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless']])\n",
    "y = np.asarray(df['churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (160, 9) (160,)\n",
      "Train set:  (40, 9) (40,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4)\n",
    "print(\"Train set: \", X_train.shape, y_train.shape)\n",
    "print(\"Train set: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (Logistic Regression with Skitit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, solver='liblinear')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C=0.01, solver = 'liblinear').fit(X_train, y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict_proba**  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86027354, 0.13972646],\n",
       "       [0.86440866, 0.13559134],\n",
       "       [0.75965295, 0.24034705],\n",
       "       [0.90541415, 0.09458585],\n",
       "       [0.7302045 , 0.2697955 ],\n",
       "       [0.52718878, 0.47281122],\n",
       "       [0.62686342, 0.37313658],\n",
       "       [0.86761127, 0.13238873],\n",
       "       [0.42055663, 0.57944337],\n",
       "       [0.90476351, 0.09523649],\n",
       "       [0.90852656, 0.09147344],\n",
       "       [0.94344351, 0.05655649],\n",
       "       [0.47142991, 0.52857009],\n",
       "       [0.50947208, 0.49052792],\n",
       "       [0.86131347, 0.13868653],\n",
       "       [0.83188719, 0.16811281],\n",
       "       [0.69567573, 0.30432427],\n",
       "       [0.83047245, 0.16952755],\n",
       "       [0.7685668 , 0.2314332 ],\n",
       "       [0.64272495, 0.35727505],\n",
       "       [0.8535616 , 0.1464384 ],\n",
       "       [0.64838979, 0.35161021],\n",
       "       [0.933414  , 0.066586  ],\n",
       "       [0.55663167, 0.44336833],\n",
       "       [0.78998604, 0.21001396],\n",
       "       [0.95194489, 0.04805511],\n",
       "       [0.76100903, 0.23899097],\n",
       "       [0.80282225, 0.19717775],\n",
       "       [0.59588458, 0.40411542],\n",
       "       [0.9647679 , 0.0352321 ],\n",
       "       [0.90626564, 0.09373436],\n",
       "       [0.88183706, 0.11816294],\n",
       "       [0.41859865, 0.58140135],\n",
       "       [0.91540455, 0.08459545],\n",
       "       [0.87914989, 0.12085011],\n",
       "       [0.753326  , 0.246674  ],\n",
       "       [0.38053   , 0.61947   ],\n",
       "       [0.5639885 , 0.4360115 ],\n",
       "       [0.96950866, 0.03049134],\n",
       "       [0.8585328 , 0.1414672 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_prob = LR.predict_proba(X_test)\n",
    "yhat_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### jaccard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486487"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test,yhat, pos_label =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 12]\n",
      " [ 1 24]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, yhat, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 3 12]\n",
      " [ 1 24]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.96      0.79        25\n",
      "           1       0.75      0.20      0.32        15\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.71      0.58      0.55        40\n",
      "weighted avg       0.70      0.68      0.61        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the count of each section, we can calculate precision and recall of each label:\n",
    "\n",
    "*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n",
    "\n",
    "*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n",
    "\n",
    "So, we can calculate the precision and recall of each class.\n",
    "\n",
    "**F1 score:**\n",
    "Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n",
    "\n",
    "The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "Finally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 0.72 in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5658910614888153"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss: : 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salaudeen Ibrahim\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)\n",
    "yhat_prob2 = LR2.predict_proba(X_test)\n",
    "print (\"LogLoss: : %.2f\" % log_loss(y_test, yhat_prob2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
